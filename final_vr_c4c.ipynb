{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fea2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Milvus server at port 19530\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from towhee import ops, pipe, register\n",
    "from towhee.operator import PyOperator\n",
    "from towhee import DataCollection\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from helpers import milvus_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "# Files\n",
    "MSRVTT_SAMPLES = \"./MSRVTT_1K.csv\"\n",
    "# file created using raw FIRE judgements, see clean_fire_judgements.ipynb\n",
    "FIRE_BENCHMARK_Q_JUDGEMENTS = \"./fire_benchmark_q_judgements.csv\" \n",
    "\n",
    "# Database Collections\n",
    "VIDEO_RET_COLLECTION = \"msrvtt_vid_ret_1\"\n",
    "FRAME_RET_COLLECTION = \"msrvtt_frame_ret_1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8cf2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "video_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentence",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8cb16268-a4ad-4ed1-a4d1-1560a1c0ae21",
       "rows": [
        [
         "0",
         "video7579",
         "./test_1k_compress/video7579.mp4",
         "a girl wearing red top and black trouser is putting a sweater on a dog"
        ],
        [
         "1",
         "video7725",
         "./test_1k_compress/video7725.mp4",
         "young people sit around the edges of a room clapping and raising their arms while others dance in the center during a party"
        ],
        [
         "2",
         "video9258",
         "./test_1k_compress/video9258.mp4",
         "a person is using a phone"
        ],
        [
         "3",
         "video7365",
         "./test_1k_compress/video7365.mp4",
         "cartoon people are eating at a restaurant"
        ],
        [
         "4",
         "video8068",
         "./test_1k_compress/video8068.mp4",
         "a woman on a couch talks to a a man"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video7579</td>\n",
       "      <td>./test_1k_compress/video7579.mp4</td>\n",
       "      <td>a girl wearing red top and black trouser is pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video7725</td>\n",
       "      <td>./test_1k_compress/video7725.mp4</td>\n",
       "      <td>young people sit around the edges of a room cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video9258</td>\n",
       "      <td>./test_1k_compress/video9258.mp4</td>\n",
       "      <td>a person is using a phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video7365</td>\n",
       "      <td>./test_1k_compress/video7365.mp4</td>\n",
       "      <td>cartoon people are eating at a restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video8068</td>\n",
       "      <td>./test_1k_compress/video8068.mp4</td>\n",
       "      <td>a woman on a couch talks to a a man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id                        video_path  \\\n",
       "0  video7579  ./test_1k_compress/video7579.mp4   \n",
       "1  video7725  ./test_1k_compress/video7725.mp4   \n",
       "2  video9258  ./test_1k_compress/video9258.mp4   \n",
       "3  video7365  ./test_1k_compress/video7365.mp4   \n",
       "4  video8068  ./test_1k_compress/video8068.mp4   \n",
       "\n",
       "                                            sentence  \n",
       "0  a girl wearing red top and black trouser is pu...  \n",
       "1  young people sit around the edges of a room cl...  \n",
       "2                          a person is using a phone  \n",
       "3          cartoon people are eating at a restaurant  \n",
       "4                a woman on a couch talks to a a man  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_samples_df = pd.read_csv(MSRVTT_SAMPLES)\n",
    "raw_samples_df[['video_id', 'video_path', 'sentence']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Collection>:\n",
       "-------------\n",
       "<name>: msrvtt_vid_ret_1\n",
       "<description>: video retrieval\n",
       "<schema>: {'auto_id': False, 'description': 'video retrieval', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 512}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_utils.create_milvus_collection(VIDEO_RET_COLLECTION, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6776d9b",
   "metadata": {},
   "source": [
    "We create a pipeline that loads the video embeddings into the Milvus Vector DB using a distributed Twohee pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f108b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_loader_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield int(line['video_id'][len('video'):]), line['video_path']\n",
    "\n",
    "video_loader_pipeline = (\n",
    "    pipe.input('csv_file')\n",
    "    .flat_map('csv_file', ('video_id', 'video_path'), read_loader_csv)\n",
    "    # Create 12 evenly distributed frames per video\n",
    "    .map('video_path', 'frames', ops.video_decode.ffmpeg(sample_type='uniform_temporal_subsample', args={'num_samples': 12}))\n",
    "    # I have a M2 Max, so device is set to mps for better performance\n",
    "    .map('frames', 'vec', ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='video', device='mps'))\n",
    "    .map(('video_id', 'vec'), (), ops.ann_insert.milvus_client(collection_name=VIDEO_RET_COLLECTION))\n",
    "    .output('video_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98936d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:04:15,510 - 17173606400 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 15:04:15,510 - 17190432768 - node.py-node:167 - INFO: Begin to run Node-read_loader_csv-0\n",
      "2025-04-16 15:04:15,511 - 17207259136 - node.py-node:167 - INFO: Begin to run Node-video-decode/ffmpeg-1\n",
      "2025-04-16 15:04:15,511 - 17224085504 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-2\n",
      "2025-04-16 15:04:15,512 - 17240911872 - node.py-node:167 - INFO: Begin to run Node-ann-insert/milvus-client-3\n",
      "2025-04-16 15:04:15,512 - 17173606400 - node.py-node:167 - INFO: Begin to run Node-_output\n"
     ]
    }
   ],
   "source": [
    "# We call the pipeline with a CSV file containing the video paths\n",
    "video_loader_ret = video_loader_pipeline(MSRVTT_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df87faf",
   "metadata": {},
   "source": [
    "The 1000 videos are now loaded into the Milvus `VIDEO_RET_COLLECTION` collection.\n",
    "\n",
    "Now, we query these videos using the annotated sentences as queries and the video ids as the ground truth results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036050f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_search_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield line['video_id'], line['sentence']\n",
    "\n",
    "video_search_pipeline = (\n",
    "    pipe.input('csv_file')\n",
    "    .flat_map('csv_file', ('rel_video_id', 'query'), read_video_search_csv)\n",
    "    .map('query', 'vec', ops.video_text_embedding.clip4clip(model_name='clip_vit_b32', modality='text', device='mps'))\n",
    "    .map('vec', 'top10_raw_res', \n",
    "         ops.ann_search.milvus_client(collection_name=VIDEO_RET_COLLECTION, limit=10))\n",
    "    .map('top10_raw_res', ('top1', 'top5', 'top10'), lambda x: (x[:1], x[:5], x[:10]))\n",
    "    .output('rel_video_id', 'query', 'top1', 'top5', 'top10')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dc47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 15:12:12,149 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 15:12:12,150 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 15:12:12,150 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 15:12:12,150 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 15:12:12,150 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 15:12:12,151 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 16:26:41,865 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 16:26:41,865 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 16:26:41,865 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 16:26:41,865 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 16:26:41,865 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 16:26:41,865 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n",
      "2025-04-16 16:26:41,869 - 17842597888 - node.py-node:142 - INFO: read_video_search_csv-0 ends with status: NodeStatus.FAILED\n",
      "2025-04-16 16:27:03,120 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 16:27:03,120 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 16:27:03,120 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 16:27:03,121 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 16:27:03,121 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 16:27:03,122 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n",
      "2025-04-16 16:27:03,127 - 17842597888 - node.py-node:142 - INFO: read_video_search_csv-0 ends with status: NodeStatus.FAILED\n",
      "2025-04-16 16:27:25,676 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 16:27:25,676 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 16:27:25,676 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 16:27:25,676 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 16:27:25,677 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 16:27:25,677 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n",
      "2025-04-16 16:43:40,226 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 16:43:40,227 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 16:43:40,227 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 16:43:40,227 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 16:43:40,227 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 16:43:40,228 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n",
      "2025-04-16 17:42:44,837 - 17800654848 - node.py-node:167 - INFO: Begin to run Node-_input\n",
      "2025-04-16 17:42:44,838 - 17842597888 - node.py-node:167 - INFO: Begin to run Node-read_video_search_csv-0\n",
      "2025-04-16 17:42:44,838 - 17976815616 - node.py-node:167 - INFO: Begin to run Node-video-text-embedding/clip4clip-1\n",
      "2025-04-16 17:42:44,839 - 19253981184 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
      "2025-04-16 17:42:44,839 - 19273904128 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
      "2025-04-16 17:42:44,839 - 21981327360 - node.py-node:167 - INFO: Begin to run Node-_output\n"
     ]
    }
   ],
   "source": [
    "all_query_results = DataCollection(video_search_pipeline(MSRVTT_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c718cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border-collapse: collapse;\"><tr><th style=\"text-align: center; font-size: 130%; border: none;\">rel_video_id</th> <th style=\"text-align: center; font-size: 130%; border: none;\">query</th> <th style=\"text-align: center; font-size: 130%; border: none;\">top1</th> <th style=\"text-align: center; font-size: 130%; border: none;\">top5</th> <th style=\"text-align: center; font-size: 130%; border: none;\">top10</th></tr>\n",
       "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">video7579</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">a girl wearing red top and black trouser is putting a sweater on a dog</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7579, 1.415151834487915]] len=1</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7579, 1.415151834487915],[9969, 1.4799106121063232],[8837, 1.4897732734680176],[9347, 1.4948583841323853],...] len=5</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7579, 1.415151834487915],[9969, 1.4799106121063232],[8837, 1.4897732734680176],[9347, 1.4948583841323853],...] len=10</td></tr>\n",
       "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">video7725</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">young people sit around the edges of a room clapping and raising their arms while others dance in the center during a party</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7725, 1.3622068166732788]] len=1</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7725, 1.3622068166732788],[8014, 1.4865269660949707],[8339, 1.4922078847885132],[8442, 1.5024112462997437],...] len=5</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7725, 1.3622068166732788],[8014, 1.4865269660949707],[8339, 1.4922078847885132],[8442, 1.5024112462997437],...] len=10</td></tr>\n",
       "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">video9258</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">a person is using a phone</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[9258, 1.4011969566345215]] len=1</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[9258, 1.4011969566345215],[9257, 1.422863483428955],[9697, 1.4413853883743286],[7910, 1.4945621490478516],...] len=5</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[9258, 1.4011969566345215],[9257, 1.422863483428955],[9697, 1.4413853883743286],[7910, 1.4945621490478516],...] len=10</td></tr>\n",
       "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">video7365</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">cartoon people are eating at a restaurant</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7365, 1.4027695655822754]] len=1</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7365, 1.4027695655822754],[8781, 1.4623048305511475],[9537, 1.4739768505096436],[7831, 1.505112648010254],...] len=5</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7365, 1.4027695655822754],[8781, 1.4623048305511475],[9537, 1.4739768505096436],[7831, 1.505112648010254],...] len=10</td></tr>\n",
       "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">video8068</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">a woman on a couch talks to a a man</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7162, 1.4716743230819702]] len=1</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7162, 1.4716743230819702],[8304, 1.4787472486495972],[8068, 1.4926888942718506],[7724, 1.4982554912567139],...] len=5</td><td style=\"text-align: left; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">[[7162, 1.4716743230819702],[8304, 1.4787472486495972],[8068, 1.4926888942718506],[7724, 1.4982554912567139],...] len=10</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_query_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce65802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Twohee data collection to a pandas dataframe so we can apply evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238195e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import eval_utils\n",
    "# TODO rerun these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fb45f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helpers.eval_utils' has no attribute 'twohee_data_col_to_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_query_results_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtwohee_data_col_to_df\u001b[49m(all_query_results)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create ground truth column as video id casted into an int, this is used for the eval functions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m all_query_results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_query_results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel_video_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m):]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'helpers.eval_utils' has no attribute 'twohee_data_col_to_df'"
     ]
    }
   ],
   "source": [
    "all_query_results_df = twohee_data_col_to_df(all_query_results)\n",
    "# Create ground truth column as video id casted into an int, this is used for the eval functions\n",
    "all_query_results_df['ground_truth'] = all_query_results_df['rel_video_id'].apply(lambda x: int(x[len('video'):]))\n",
    "all_query_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71f35bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metrics at top-1, 5, 10:\n",
      "{'recall@1': 0.426, 'recall@5': 0.716, 'recall@10': 0.814}\n",
      "Mean Average Precision (MAP):\n",
      "0.5456543650793645\n"
     ]
    }
   ],
   "source": [
    "recall_metrics = calculate_recall(all_query_results_df)\n",
    "map_value = calculate_mean_average_precision(all_query_results_df)\n",
    "\n",
    "print(\"Recall metrics at top-1, 5, 10:\")\n",
    "print(recall_metrics)\n",
    "print(\"Mean Average Precision (MAP):\")\n",
    "print(map_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e467cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@1: 0.426\n",
      "NDCG@5: 0.5780321865313451\n",
      "NDCG@10: 0.6100154270801753\n"
     ]
    }
   ],
   "source": [
    "# Calculate NDCG with k=1, 5, 10\n",
    "for k in [1, 5, 10]:\n",
    "    ndcg_value = calculate_ndcg(all_query_results_df, k=k)\n",
    "    print(f\"NDCG@{k}: {ndcg_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce772ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': 0.426,\n",
       " 'recall@5': 0.716,\n",
       " 'recall@10': 0.814,\n",
       " 'map': 0.5456543650793645,\n",
       " 'ndcg@1': 0.426,\n",
       " 'ndcg@5': 0.5780321865313451,\n",
       " 'ndcg@10': 0.6100154270801753}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_eval_scores(all_query_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac418fe3",
   "metadata": {},
   "source": [
    "## Try evaluation against queries from FIRE benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7b881",
   "metadata": {},
   "source": [
    "We are working with a sample of MSR-VTT and our evaluation pipeline supports only one relevant query per video, hence we need to filter the full FIRE benchmark to only include videos we have sampled and ones with a single relevant result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbc9cd",
   "metadata": {},
   "source": [
    "FIRE_BENCHMARK_Q_JUDGEMENTS is created in the notebook `./clean_fire_judgements.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4409ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query pipeline using FIRE\n",
    "\n",
    "def read_frame_search_fire_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield line['video_id'], line['sentence']\n",
    "            \n",
    "fire_query_results = DataCollection(video_search_pipeline(FIRE_BENCHMARK_Q_JUDGEMENTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f552035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@1': 0.5732484076433121,\n",
       " 'recall@5': 0.821656050955414,\n",
       " 'recall@10': 0.9076433121019108,\n",
       " 'map': 0.6800513092710544,\n",
       " 'ndcg@1': 0.5732484076433121,\n",
       " 'ndcg@5': 0.7064313418462581,\n",
       " 'ndcg@10': 0.7348170561659407}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_eval_scores(twohee_data_col_to_df(fire_query_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-ret-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
